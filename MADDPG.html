<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Application of the algorithm described in OpenAI's paper "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments."</title>
    <link rel="icon" href="/favicon.ico">
    <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@400;700&family=Roboto:wght@400;500&display=swap" rel="stylesheet">

    <style>
        /* Global Reset */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Roboto', sans-serif;
            background-color: #fafafa;
            color: #333;
            line-height: 1.6;
            padding: 40px 20px;
        }

        /* Container */
        .container {
            max-width: 1100px;
            margin: 40px auto;
            background-color: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 0 15px rgba(0, 0, 0, 0.05);
        }

        /* Header */
        .header {
            text-align: center;
            margin-bottom: 40px;
            font-family: 'Merriweather', serif;
        }

        .header h1 {
            font-size: 2.5rem;
            font-weight: 800;
            margin-bottom: 10px;
            letter-spacing: -1px;
            color: #1a202c;
        }

        .header p {
            color: #555;
            font-size: 1.2rem;
        }

        /* Section Styles */
        .section-title {
            font-family: 'Merriweather', serif;
            color: #1a202c;
            font-size: 1.4rem;
            font-weight: bold;
            color: #2b3a42;
            margin-top: 30px;
            border-bottom: 0.2px solid #000000;
            padding-bottom: 5px;
            margin-bottom: 15px;
        }

        p {
            font-size: 1.1rem;
            margin-bottom: 20px;
            color: #555;
            line-height: 1.6;
        }

        /* New List Styles */
        p + ul, p + ol {
            margin-top: -15px;
            margin-bottom: 20px;
            padding-left: 40px;
        }

        .section ul, .section ol {
            margin-bottom: 20px;
            line-height: 1.6;
            color: #555;
        }

        .section li {
            font-size: 1.1rem;
            margin-bottom: 8px;
        }

        .authors {
            text-align: center;
            margin-bottom: 40px;
        }

        .authors table {
            margin: 0 auto;
            border-collapse: collapse;
        }

        .authors td {
            padding: 5px 15px;
        }

        .authors a {
            text-decoration: none;
            color: #2b3a42;
            font-weight: 500;
        }

        .authors a:hover {
            text-decoration: underline;
        }

        .section img {
            max-width: 100%;
            border-radius: 8px;
            margin-bottom: 20px;
            box-shadow: 0 0 12px rgba(0, 0, 0, 0.1);
        }

        .code-block {
            background-color: #282c34;
            padding: 20px;
            border-radius: 8px;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
            margin-top: 30px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }

        .code-block code {
            display: block;
            white-space: pre-wrap;
            color: #abb2bf;
            line-height: 1.5;
        }

        /* Responsive */
        @media (max-width: 768px) {
            .header h1 {
                font-size: 2rem;
            }

            .container {
                padding: 20px;
            }

            .section-title {
                font-size: 1.6rem;
            }

            .video-container iframe {
                height: 220px;
            }
        }
    </style>
</head>

<body>
    <div class="container">
        <!-- Header Section -->
        <div class="header">
            <h1>Application of the algorithm described in OpenAI's paper "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"</h1>
        </div>

        <!-- Authors and Affiliations -->
        <div class="authors">
            <table>
                <tr>
                    <td>Abdulloh</td>
                </tr>
            </table>
        </div>

        <!-- Abstract Section -->
        <section>
            <div class="section-title">Abstract</div>
            <p>This document details the implementation of Multi-Agent Deep Deterministic Policy Gradient (MADDPG), a decentralized multi-agent reinforcement learning algorithm, for cooperative environments. We provide a comprehensive explanation of the algorithm's components, including the network architectures, training procedure, and replay buffer mechanism. The provided Python code implements MADDPG for the "simple spread" scenario from the multi-agent particle environment. We discuss design choices, potential extensions, and areas for future research. Furthermore, we reference and build upon OpenAI's work on Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments (Lowe et al., 2017), incorporating key insights from their approach into this implementation. The system was tested on the "simple spread" environment, as defined in the multi-agent particle environment setup.</p>
        </section>

        <!-- Introduction Section -->
        <section>
            <div class="section-title">Introduction</div>
            <p>Multi-agent reinforcement learning (MARL) presents unique challenges compared to single-agent RL due to the dynamic nature of the environment caused by the presence of multiple learning agents. MADDPG addresses the non-stationarity issue by enabling agents to learn decentralized policies while leveraging centralized information during training. This approach allows agents to reason about the actions of other agents, leading to improved coordination and performance.</p>
        </section>

        <!-- Objectives Section -->
        <section>
            <div class="section-title">Objectives</div>
            <p>The primary objective of this implementation is to demonstrate the effectiveness of MADDPG in training agents to effectively cooperate in the "simple spread" environment. In this scenario, agents are rewarded for spreading out and covering distinct areas of the environment while avoiding collisions. More specifically, the objectives are:</p>
            <ul>
                <li><strong>Maximize Collective Reward:</strong> Train agents to collectively maximize their shared reward, reflecting successful coverage of the environment.</li>
                <li><strong>Learn Decentralized Policies:</strong> Ensure that each agent learns a robust decentralized policy, allowing it to act independently based on local observations.</li>
                <li><strong>Promote Coordination:</strong> Observe emergent coordinated behavior between agents to achieve the shared objective despite the absence of explicit communication during policy execution.</li>
            </ul>
        </section>

        <!-- Algorithm Overview Section -->
        <section>
            <div class="section-title">Algorithm Overview</div>
            <p>MADDPG builds upon the Deep Deterministic Policy Gradient (DDPG) algorithm. Each agent learns a deterministic policy, represented by an actor network, that maps observations to actions. A centralized critic network is employed for each agent, which takes as input the global state and the actions of all agents to estimate the Q-value of joint actions. This centralized critic provides a stable learning signal for the actor, allowing it to effectively learn cooperative behaviors. This implementation draws inspiration from the work presented in OpenAI's paper "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments" (Lowe et al., 2017), which explores decentralized policies in environments with both cooperative and competitive elements.</p>
        </section>

        <!-- Implementation Details Section -->
        <section>
            <div class="section-title">Implementation Details</div>
            <p>The implementation adheres closely to the algorithmic structure outlined in Lowe et al.'s work, though it does not incorporate certain advanced techniques such as "inferring policies of other agents" and "policy ensembles." These techniques, while valuable in more complex multi-agent scenarios, are omitted in this simplified version, allowing for a more focused exploration of the MADDPG framework in cooperative environments.</p>
            <p>In our approach, each agent is trained using a decentralized policy while benefiting from centralized training signals. Specifically, during training, each agent utilizes global state information and the actions of all agents to update its policy. This allows the agent to account for the actions of others in a manner that enhances cooperative behavior in tasks where coordination is essential. The training procedure follows the pattern established in Lowe et al., but without the additional complexity of the aforementioned techniques.</p>
        </section>

        <!-- Key Design -->
        <section>
            <div class="section-title">Key Design Choices and Considerations</div>
            <p>
                <ul>
                    <li><strong>Centralized Training, Decentralized Execution:</strong> MADDPG uses centralized information during training but employs decentralized policies during execution. This allows agents to learn about the actions and intentions of other agents without requiring explicit communication during execution.</li>
                    <li><strong>Target Networks:</strong> Similar to DDPG, target networks are used for both the actor and critic to stabilize learning. The soft update mechanism gradually updates the target networks towards the learned networks.</li>
                    <li><strong>Replay Buffer:</strong> A multi-agent replay buffer stores experiences from all agents, allowing for efficient learning from past interactions.</li>
                    <li><strong>Exploration:</strong> Epsilon-greedy exploration is used in the action selection process. This ensures that agents continue to explore the environment, even as their policies converge.</li>
                </ul>
            </p>
        </section>

        <!-- Results Section -->
        <section>
            <div class="section-title">Results</div>
            <p>The performance of the MADDPG algorithm is evaluated by monitoring the episode rewards during training. The following results were observed on the "simple spread" environment:</p>
            <ul>
                <li><strong>Increasing Reward Trend:</strong> The average episode reward generally exhibits an upward trend over the training period, demonstrating the learning progress of the agents. The agents progressively learn to spread out and cover the environment more efficiently.</li>
                <li><strong>Emergent Coordination:</strong> Observing the agent behavior during and after training reveals emergent coordination. Agents learn to avoid each other while simultaneously moving towards uncovered areas, indicative of implicit communication through their actions and their effect on the environment state.</li>
                <li><strong>Challenges:</strong> Despite successful learning, the agents exhibited suboptimal performance in certain configurations due to the difficulty of balancing exploration and exploitation in the decentralized setting.</li>
            </ul>
        </section>

        <!-- Future Research -->
        <div class="section-title">Future Research and Extensions</div>
        <p>
            <ul>
                <li><strong>Alternative Exploration Strategies:</strong> Investigating more sophisticated exploration strategies like Ornstein-Uhlenbeck noise or entropy-based exploration could enhance learning.</li>
                <li><strong>Recurrent Networks:</strong> Incorporating recurrent networks in the actor and critic architectures could allow agents to learn effective policies in partially observable environments.</li>
                <li><strong>Communication Protocols:</strong> Exploring explicit communication mechanisms between agents could further improve coordination and performance in complex cooperative tasks.</li>
                <li><strong>Parameter Tuning:</strong> Systematic investigation of hyperparameters like learning rates, batch size, and target network update rate could further optimize performance.</li>
            </ul>
        </p>

        <!-- Conclusion Section -->
        <section>
            <div class="section-title">Conclusion</div>
            <p>The MADDPG implementation for cooperative environments was successful in demonstrating the power of centralized training and decentralized execution. The ability to leverage centralized information during training allowed agents to achieve high levels of coordination. This work lays the foundation for future exploration of more complex multi-agent tasks and provides insights into the benefits and limitations of MADDPG in cooperative settings.</p>
        </section>

        <!-- References Section -->
        <section>
            <div class="section-title">References</div>
            <p>
                <ol>
                    <li><a href="https://arxiv.org/pdf/1706.02275.pdf">Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., & Ziebart, B. (2017). Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments. In Advances in Neural Information Processing Systems (NeurIPS). <a href="https://arxiv.org/pdf/1706.02275.pdf">PDF</a></li>
                    <li><a href="https://github.com/openai/multiagent-particle-envs">Multi-Agent Particle Environments</a></li>
                </ol>
            </p>
        </section>
    </div>
</body>

</html>
